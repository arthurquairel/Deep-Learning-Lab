# -*- coding: utf-8 -*-
"""Quairel_Arthur_DL_Conv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d9qzOOUP88BD1g53liHQq9zrkVSI9CEB

This notebook is  a follow up of the lab session on Fashion MNIST. Be sure you did and understood the previous notebook on applying the feed-forward neural network with pytorch.

The goal is to incrementally build an image classifier based on convolutional layers. Since we consider images and convolution we will use Tensors with peculiar shapes in input. Moreover, this session is also the occasion to introduce "Max-pooling" and to sue "Batch-normalization" again.




# Dataset

First get the dataset. This is the same as last lab.
"""

# math, numpy and plot
import numpy as np
import math
import matplotlib
import matplotlib.pyplot as plt
# torch
import torch as th
import torch.autograd as autograd
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# gzip
import gzip
import pickle



# To download the file:
! wget -nc --no-check-certificate "https://drive.usercontent.google.com/download?id=1qx-a3KjzX2W66Aq84tnjGRUa10CTxOJN&confirm=t" -O "./fashion-mnist.pk.gz"

# Or if you prefer from Colab:
#import gdown
#url = 'https://drive.google.com/uc?id=1qx-a3KjzX2W66Aq84tnjGRUa10CTxOJN'
#output = 'fashion-mnist.pk.gz'
#gdown.download(url, output, quiet=False)
# Load the dataset
fp = gzip.open('./fashion-mnist.pk.gz','rb')
allXtrain, allYtrain, Xvalid, Yvalid, classlist  = pickle.load(fp)

"""##  Data format

The provided version is unfortunately not adapted to our purpose in terms of dimensions.
*Convolution2D* expects as input a Tensor with 4 dimensions $(N,C,H,W)$ with :

- N the batch dimension, *i.e* the number of images
- C the number of input channels, here it is 1
- H the height or number of rows of each image
- W the width  or number of columns of each image

## DataSet and DataLoader

In pytorch data handling is done in 2 steps:
- `DataSet`: a class to access the raw data, it can be tensors, files, distributed files, ...
- `DataLoader` (the class to iterate through the dataset and to get access to well prepared batch of data)

From the model viewpoint:
- During training and testing the model interacts with the `DataLoader` to go through the `DataSet`
- The Dataloader pick what is necessary in the `DataSet`  

Here we have Tensors, so we can use a `TensorDataSet`.  This abstraction for handling data is important. Depending on the application and the data types, it is very easier in practice to divide the process in two steps: the Dataset and the interaction between the model and the data via the dataset.
"""

BatchSize = 100
splittrain = 30000
# create your datset
# dataset = TensorDataset(x_tensor, y_tensor)
trainset = TensorDataset(allXtrain[:splittrain].view(splittrain,1,28,28)/255, allYtrain[:splittrain])
# create your dataloader
# if you have also labels you do
trainloader = DataLoader(trainset,batch_size=BatchSize, shuffle=True)
# Do the same for the validation set
validset = TensorDataset(allXtrain[splittrain:].view(splittrain,1,28,28)/255, allYtrain[splittrain:])
validloader = DataLoader(validset,batch_size=BatchSize, shuffle=True)

"""To look at one batch :"""

batch = next(iter(trainloader))
# Explore what you get as a batch

"""# Playing with convolution in 2D

Let start the exploration of convolution.The class we will use is called Conv2d. Read carefully the documentation of this module. Maybe you cannot understand everything. That's why it is useful to first play with the convolution with one image.
"""

img = allXtrain[0]
print(img.unsqueeze(0))




plt.imshow(img.reshape(28,28))

"""Now create one convolution layer with 1 input channels, a kernel size of 3, and a stride of 1.
Try it and look at the output dimension.

"""

Conv=nn.Conv2d(in_channels=1,out_channels=3,kernel_size=3,stride=1)


img=img.reshape(1,1,28,28)

output=Conv(img)

print(f"Shape de l'image d'entrée : {img.shape}")
print(f"Shape de l'image après convolution : {output.shape}")

"""The resulting "image" is not of the same dimension, how to obtain an output with the same dimension (same question with a kernel size of 5) ?"""

Conv=nn.Conv2d(in_channels=1,out_channels=3,kernel_size=5,stride=1,padding=2)

output=Conv(img)

print(f"Shape de l'image d'entrée : {img.shape}")
print(f"Shape de l'image après convolution : {output.shape}")

"""We can define the parameters of the convolutional filter with our own hands. For that purpose we just have to create the Tensor we want and cast it in a *Parameter* object (usefull for autograd) and then assign it.
This is an example:
"""

# Create a convolutional filter
convFilter = nn.Conv2d(in_channels=1,out_channels=1,
                       kernel_size = 6, padding=3,
                       stride=1)
# build the weight matrix you want
W=th.ones(convFilter.weight.shape)
# Makes it a Parameter and assign
convFilter.weight = nn.Parameter(W)

res = convFilter(img)
plt.subplot(1,2,1)
plt.imshow(img.squeeze(),  matplotlib.pyplot.cm.gray)
plt.subplot(1,2,2)
plt.imshow(F.relu(res).squeeze().detach(), matplotlib.pyplot.cm.gray)

"""Now try to set the convolution fiter as follows:
$$
\left(
\begin{array}{ccc}
 -1 &2&-1\\
 -1 &2&-1\\
 -1 &2&-1
\end{array}
\right)
$$
and then as follows:
$$
\left(
\begin{array}{rrr}
 -1 &-1&-1\\
 2 &2&2\\
 -1 &-1&-1
\end{array}
\right)
$$
Try them on some images and visualize the results.
"""

# TODO

W1=th.tensor([[-1,2,-1],[-1,2,-1],[-1,2,-1]],dtype=th.float32)
W=W.reshape(1,1,3,3)
print(W.dtype,img.dtype)
convFilter.weight = nn.Parameter(W1)

res = convFilter(img)
plt.subplot(1,2,1)
plt.imshow(img.squeeze(),  matplotlib.pyplot.cm.gray)
plt.subplot(1,2,2)
plt.imshow(F.relu(res).squeeze().detach(), matplotlib.pyplot.cm.gray)


W=th.tensor([[-1,-1,-1],[2,2,2],[-1,-1,-1]],dtype=th.float32)
W=W.reshape(1,1,3,3)
convFilter.weight = nn.Parameter(W)

res = convFilter(img)
plt.subplot(1,2,1)
plt.imshow(img.squeeze(),  matplotlib.pyplot.cm.gray)
plt.subplot(1,2,2)
plt.imshow(F.relu(res).squeeze().detach(), matplotlib.pyplot.cm.gray)

"""# Pool !

Now we introduce the max-pooling in 2 dimensions: *MaxPool2d*. Look at the documentation and then try to define the following pipeline:
- a convolution with a kernel size of 3, stride 1 and padding 1
- apply the ReLu function and
- a max pooling with kernel size of 2 and a stride of 2.
Try to guess before running your code the dimensions of the output !

"""

maxpool=nn.MaxPool2d(kernel_size=2,stride=2)
conv=nn.Conv2d(in_channels=1,out_channels=1,kernel_size=6,stride=3,padding=)
out=conv(img)
out=F.relu(out)
out=maxpool(out)
print(f"Shape de l'image d'entrée : {img.shape}")
print(f"Shape de l'image après convolution : {out.shape}")

"""#  A first model

After this interlude, the goal now is to write a class to implement the model with:

- 2D convolution with (kernel size = 3, padding = 1, stride 1)
- ReLu activation
- Max-pooling (kernel size = 2, stride 2)
- A final linear classifier
- The final activation

Writing this class, allows you to wrap what you have seen so far. To debug the model, you can first play step-by-step with each layer to ensure you obtain the right dimensions (it was done earlier). Then, write the class and run the training to evaluate the result (this what we have to do now).

The class inherits from an existing class of pytorch : *Module*. This mean: it is a *Module*, but we add some peculiarities. For that purpose we can fill the following code:
"""

# TODO
class FashionCNN(nn.Module):

    def __init__(self, kernel_size = 3, padding= 1, out_channels = 1):
        super(FashionCNN, self).__init__()
        # TODO : write the end of the constructor.
        # It is important to create here all the layers of the network.
        # All layers that have paramaters should be attribute.
        # For example:
        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels,
                              kernel_size=kernel_size, padding=padding)
        # TODO: add the rest
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten=nn.Flatten(start_dim=-3,end_dim=-1)
        self.linear=nn.Linear(in_features=out_channels*14*14,out_features=10)
        self.activ=nn.LogSoftmax(dim=1)
        self.batchnorm=nn.BatchNorm2d(out_channels)

    def forward(self, x):
        # TODO
        # if you need to run forward with the conv layer,
        # you can call it by self.conv
        outconv1  = self.conv(x)
        outrelu1 = self.relu(outconv1)
        outpool1 = self.maxpool(outrelu1)
        outbatch1=self.batchnorm(outpool1)
        outconv2  = self.conv(outbatch1)
        outrelu2 = self.relu(outconv2)
        outbatch2=self.batchnorm(outrelu2)
        outflatten=self.flatten(outbatch2)
        outlinear=self.linear(outflatten)
        output=self.activ(outlinear)
        return output

# Test the class: is everything in place:
# A first classifier is built like :
classif = FashionCNN()
# The parameters of the classifier are randomly initialize, but we
# can use it on a image :
out = classif.forward(img)
print(out.shape) # the output has 2 dimensions
print(out)

# It is correct ? If not, correct the class to get the expected result.

"""# Training the model

To train the model, we need to define a loss function and an optimizer. For the moment we will rely on an online learning algorithm: online stochastic gradient descent. Like the previous lab session:
- we pick one training example
- compute the loss
- back-propagation of the gradient
- update of the parameters


At the end of one epoch, we evaluate the model on the validation step. You can use for that purpose the training function we wrote earlier.


To train the CNN, we can reuse the training function you wrote carefully in the previous lab session. However we need to adapt it in order to use dataloaders (the `trainloader` and `validloader`)

Question:
- As optimizer we will use *Adam*. It is important to find the good choice of hyper-parameter for the initial learning rate. Try different values like 0.1, 0.01, ...
- Then try with a number of output channel set to 1, 8, 16.


"""

# TODO : paste here the training function you wrote before.
# Of course you can improve it if you want during the lab session.
# Especially in terms of parameters.
def train_loop(model,loss_fn,optimizer,trainloader,validloader,Nepoch):
  loss_history = []

  loss_valid=[]
  accuracy_training=[]
  accuracy_validation=[]

  for epoch in range(Nepoch):
    loss_epoch=[]
    accuracy_epoch=[]

    for Xtrain,Ytrain in trainloader:
      preds=model(Xtrain)
      loss=loss_fn(preds,Ytrain)
      loss_epoch.append(loss.item())
      accuracy_epoch2 = (preds.argmax(dim=1) == Ytrain).float().mean()
      accuracy_epoch2 = accuracy_epoch2.item()
      accuracy_training.append(accuracy_epoch2)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    for Xvalid,Yvalid in validloader:
      preds=model.forward(Xvalid)
      loss=loss_fn(preds,Yvalid)
      loss_valid.append(loss.item())
      accuracy_epoch3 = (preds.argmax(dim=1) == Yvalid).float().mean()
      accuracy_epoch3 = accuracy_epoch3.item()
      accuracy_epoch.append(accuracy_epoch3)

    loss_history.append(np.mean(loss_epoch))
    accuracy_validation.append(np.mean(accuracy_epoch))



  print(accuracy_epoch)
  return loss,preds,loss_history,loss_valid,accuracy_validation,accuracy_training

loss_fn = nn.NLLLoss()
model = FashionCNN()
L,Y,loss_training,loss_valid,acc_valid,acc_train=train_loop(model,loss_fn,th.optim.Adam(model.parameters(),lr=0.001),trainloader,validloader,10)

plt.figure(figsize=(10, 5))
plt.plot(loss_training, label='loss training')
#plt.plot(loss_valid, label='loss validation')

plt.title("Évolution des différentes loss")
plt.xlabel("Nombre d'itération")
plt.ylabel("Perte")
plt.legend()
plt.grid(True)


plt.figure(figsize=(10, 5))
plt.plot(acc_train, label='Training accuracy')
#plt.plot(acc_valid, label='Validation accuracy')
plt.title("Évolution des différentes accuracy ")
plt.xlabel("Nombre d'itération")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(loss_training, label='loss validation')
plt.title("Évolution des différentes loss")
plt.xlabel("Nombre d'itération")
plt.ylabel("Perte")
plt.legend()
plt.grid(True)

# TODO
loss_fn=nn.NLLLoss()
model = FashionCNN()
# Train you model !

"""## Batch-norm

Extend your model to include the Batch-normalization.

"""



"""## More convolution

We can now define an extended model where the basic block is : Conv2D, ReLu, BatchNorm and MaxPool.
We stack two blocks of this kind before the classification.

For instance in the previous model, this kind of block reduce the image size and increase the number of output channels. We can try to do the same and double this number in the second block.

TODO:
- Implement a model with two blocks as decribed above.
- We can then improve the output classifier.
- Play with the hyper-parameters.

Of course if you want to leverage a deeper model it is useful to increase the amount of training data (we only take the first 30k images until now).
"""

# TODO : paste here the training function you wrote before.
# Of course you can improve it if you want during the lab session.
# Especially in terms of parameters.
def train_loop(model,loss_fn,optimizer,trainloader,validloader,Nepoch):
  loss_history = []

  loss_valid=[]
  accuracy_training=[]
  accuracy_validation=[]

  for epoch in range(Nepoch):
    loss_epoch=[]
    accuracy_epoch=[]

    for Xtrain,Ytrain in trainloader:
      preds=model(Xtrain)
      loss=loss_fn(preds,Ytrain)
      loss_epoch.append(loss.item())
      accuracy_epoch2 = (preds.argmax(dim=1) == Ytrain).float().mean()
      accuracy_epoch2 = accuracy_epoch2.item()
      accuracy_training.append(accuracy_epoch2)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    for Xvalid,Yvalid in validloader:
      preds=model.forward(Xvalid)
      loss=loss_fn(preds,Yvalid)
      loss_valid.append(loss.item())
      accuracy_epoch3 = (preds.argmax(dim=1) == Yvalid).float().mean()
      accuracy_epoch3 = accuracy_epoch3.item()
      accuracy_epoch.append(accuracy_epoch3)

    loss_history.append(np.mean(loss_epoch))
    accuracy_validation.append(np.mean(accuracy_epoch))



  print(accuracy_epoch)
  return loss,preds,loss_history,loss_valid,accuracy_validation,accuracy_training

loss_fn = nn.NLLLoss()
model = FashionCNN()
L,Y,loss_training,loss_valid,acc_valid,acc_train=train_loop(model,loss_fn,th.optim.Adam(model.parameters(),lr=0.001),trainloader,validloader,10)

plt.figure(figsize=(10, 5))
plt.plot(loss_training, label='loss training')
#plt.plot(loss_valid, label='loss validation')

plt.title("Évolution des différentes loss")
plt.xlabel("Nombre d'itération")
plt.ylabel("Perte")
plt.legend()
plt.grid(True)


plt.figure(figsize=(10, 5))
plt.plot(acc_train, label='Training accuracy')
#plt.plot(acc_valid, label='Validation accuracy')
plt.title("Évolution des différentes accuracy ")
plt.xlabel("Nombre d'itération")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

"""# And now in color: CIFAR-10


To experiment image classification on a coloured image, we can use the CIFAR-10 dataset.
You can find more details for instance on this page: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html. We can download the dataset with a dataloader directly:
"""

import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 16


trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = th.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = th.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

"""With this example we will use a **dataset** via a *dataloader*.  This is a convenient tool to handle datasets with efficient iterators."""

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

dataiter = iter(trainloader)
images, labels = next(dataiter)
imshow(torchvision.utils.make_grid(images))

batch = next(iter(trainloader))
print(batch[0].shape)

"""# Todo

Implement `VGG 16` architecture to get state of the art performance (see the course for the architecture)
"""

# TODO
class FashionCNN(nn.Module):

    def __init__(self, kernel_size = 3, padding= 1, out_channels = 1):
        super(FashionCNN, self).__init__()
        # TODO : write the end of the constructor.
        # It is important to create here all the layers of the network.
        # All layers that have paramaters should be attribute.
        # For example:
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=out_channels,
                              kernel_size=kernel_size, padding=padding)
        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,
                              kernel_size=kernel_size, padding=padding)
        # TODO: add the rest
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten=nn.Flatten(start_dim=-3,end_dim=-1)
        self.linear=nn.Linear(in_features=out_channels*16*16,out_features=10)
        self.activ=nn.LogSoftmax(dim=1)
        self.batchnorm=nn.BatchNorm2d(out_channels)

    def forward(self, x):
        # TODO
        # if you need to run forward with the conv layer,
        # you can call it by self.conv
        outconv1  = self.conv1(x)
        outrelu1 = self.relu(outconv1)
        outpool1 = self.maxpool(outrelu1)
        outbatch1=self.batchnorm(outpool1)
        outconv2  = self.conv2(outbatch1)
        outrelu2 = self.relu(outconv2)
        outbatch2=self.batchnorm(outrelu2)
        outflatten=self.flatten(outbatch2)
        outlinear=self.linear(outflatten)
        output=self.activ(outlinear)
        return output

# TODO : paste here the training function you wrote before.
# Of course you can improve it if you want during the lab session.
# Especially in terms of parameters.
def train_loop(model,loss_fn,optimizer,trainloader,validloader,Nepoch):
  loss_history = []

  loss_valid=[]
  accuracy_training=[]
  accuracy_validation=[]

  for epoch in range(Nepoch):
    loss_epoch=[]
    accuracy_epoch=[]

    for Xtrain,Ytrain in trainloader:

      preds=model(Xtrain)
      loss=loss_fn(preds,Ytrain)
      loss_epoch.append(loss.item())
      accuracy_epoch2 = (preds.argmax(dim=1) == Ytrain).float().mean()
      accuracy_epoch2 = accuracy_epoch2.item()
      accuracy_training.append(accuracy_epoch2)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    for Xvalid,Yvalid in validloader:
      preds=model.forward(Xvalid)
      loss=loss_fn(preds,Yvalid)
      loss_valid.append(loss.item())
      accuracy_epoch3 = (preds.argmax(dim=1) == Yvalid).float().mean()
      accuracy_epoch3 = accuracy_epoch3.item()
      accuracy_epoch.append(accuracy_epoch3)

    loss_history.append(np.mean(loss_epoch))
    accuracy_validation.append(np.mean(accuracy_epoch))



  print(accuracy_epoch)
  return loss,preds,loss_history,loss_valid,accuracy_validation,accuracy_training

loss_fn = nn.NLLLoss()
model = FashionCNN()
L,Y,loss_training,loss_valid,acc_valid,acc_train=train_loop(model,loss_fn,th.optim.Adam(model.parameters(),lr=0.001),trainloader,testloader,10)

plt.figure(figsize=(10, 5))
plt.plot(loss_training, label='loss training')
#plt.plot(loss_valid, label='loss validation')

plt.title("Évolution des différentes loss")
plt.xlabel("Nombre d'itération")
plt.ylabel("Perte")
plt.legend()
plt.grid(True)


plt.figure(figsize=(10, 5))
plt.plot(acc_train, label='Training accuracy')
#plt.plot(acc_valid, label='Validation accuracy')
plt.title("Évolution des différentes accuracy ")
plt.xlabel("Nombre d'itération")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()